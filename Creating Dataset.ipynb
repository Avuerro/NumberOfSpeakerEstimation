{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset for Number of Speaker Estimation\n",
    "\n",
    "This notebook shows how we have created the dataset for the Number of Speaker Estimation project.\n",
    "\n",
    "We first start with importing `util.py`, where all necessary code is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src/')\n",
    "from src import util\n",
    "\n",
    "dataset = \"train100\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note is that for this code to work, LibriSpeech has to be downloaded and unzipped in a `./data/` folder. The general folder path is then:\n",
    "```\n",
    "./data/LibriSpeech/type_of_librispeech/speaker_id/chapter_id/audiofile.flac\n",
    "```\n",
    "If you have downloaded, for example, the LibriSpeech train-100 clean dataset, the following path would be valid:\n",
    "```\n",
    "./data/LibriSpeech/train-clean-100/19/198/19-198-0000.flac\n",
    "```\n",
    "\n",
    "**Please note that running this notebook requires a lot of harddisk space. At each step, we save the output as new files in a new folder. This is intentional, for testing purposes and to have a back-up of the previous steps. If you want to run this notebook withouth doing so, code in the source files has to be changed, or intermediate results should manually be removed from the harddisk.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rewrite LibriSpeech `.flac` to `.wav`\n",
    "\n",
    "We first rewrite LibriSpeech to wav files, which we do by calling `util.flacs_to_wavs`.\n",
    "This function requires two parameters:\n",
    "\n",
    " - `data_dir` represents where the original data is located. According to the beforementioned structure, this would be `./data/LibriSpeech/'\n",
    " - `new_dir` represents the new location where the `.wav` files will be written to.\n",
    "\n",
    "In the cell below, the `new_dir` is called wavs100, to indicate the file format (`.wav`) and to remind that this is from train-100 clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./../data/LibriSpeech/dev-clean\" \n",
    "wav_data = \"./../data/wavs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [00:15,  8.92it/s]\n"
     ]
    }
   ],
   "source": [
    "util.flacs_to_wavs(data_dir, wav_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split `.wav` files in fragments of 5 seconds\n",
    "\n",
    "The function `util.split_audio_in_samples` is written to do this. It requires three parameters:\n",
    "\n",
    " - `data_dir` represents where the original `.wav` files are located. According to the previous cell with code, this is `./data/wavs100/`\n",
    " - `new_dir` represents the new location where the shorter `.wav` files will be written to.\n",
    " - `t` is how long the files should be in seconds, in our case `t = 5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = wav_data\n",
    "split_data = \"./../data/splits\"\n",
    "t = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 6394.49it/s]\n",
      "/home/chb3k/Documents/KI/master/ASR/project/codebase/NumberOfSpeakerEstimation/src/util.py:96: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  files_per_speaker.append(speaker_files)\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for all speaker files\n",
      "1288 files found in total\n",
      "Creating the splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:54<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "util.create_audio_splits(data_dir, split_data, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Change Loudness of All Splits to 70 dB\n",
    "\n",
    "The function `util.change_loudness` will change the loudness of every split to 70 dB. It requires three parameters:\n",
    " - `data_dir` denotes where the splitted `.wav` files are located.\n",
    " - `new_dir` denotes where the new files will be written to.\n",
    " - `target` denotes the target level of decibels. Default is set to 70.\n",
    " \n",
    "Since we use a subprocess call in this function, this function will run a bit longer than the previous function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = split_data\n",
    "normalized_data = \"./../data/normalized/\"\n",
    "target = 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [02:32,  3.73s/it]\n"
     ]
    }
   ],
   "source": [
    "util.change_loudness(data_dir, normalized_data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merging Files\n",
    "\n",
    "For merging splits, and hence actually creating the datasets, we use the function `util.merge_splits`. It takes three parameters:\n",
    " - `data_dir` represents where the normalized split files are located.\n",
    " - `new_dir` represents the location where the merged split files will be written to\n",
    " - `max_number_of_speakers`\n",
    "\n",
    "This function is different from the others before, since this function creates another folder in `new_dir`, namely a folder named `metadata`, where `.txt` files are saved containing the speaker IDs from the speakers that are merged. The name of this `.txt` file corresponds with the merged `.wav` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = normalized_data\n",
    "new_dir = \"./data/trainset100/\"\n",
    "max_nr_of_speakers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chb3k/Documents/KI/master/ASR/project/codebase/NumberOfSpeakerEstimation/src/util.py:193: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  print(\"{} files found in total\".format(nr_of_files))\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966 files found in total\n",
      "Now starting to merge files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:02, 18.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 406 unique datapoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "util.merge_audiofiles(data_dir, merged_data, max_nr_of_speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating STFTs from Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/trainset100/train\"\n",
    "new_dir = \"./data/trainset100/stft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439/439 [00:28<00:00, 15.34it/s]\n",
      "100%|██████████| 441/441 [00:05<00:00, 75.60it/s]\n",
      "100%|██████████| 439/439 [00:27<00:00, 16.02it/s]\n",
      "100%|██████████| 440/440 [00:26<00:00, 16.47it/s]\n",
      "100%|██████████| 445/445 [00:18<00:00, 24.52it/s]\n",
      "100%|██████████| 438/438 [00:28<00:00, 15.50it/s]\n",
      "100%|██████████| 440/440 [00:27<00:00, 15.85it/s]\n",
      "100%|██████████| 443/443 [00:25<00:00, 17.46it/s]\n",
      "100%|██████████| 446/446 [00:23<00:00, 18.64it/s]\n",
      "100%|██████████| 442/442 [00:21<00:00, 20.21it/s]\n",
      "100%|██████████| 441/441 [00:26<00:00, 16.78it/s]\n",
      "12it [04:20, 21.69s/it]\n"
     ]
    }
   ],
   "source": [
    "util.create_spectrograms(data_dir, new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Done\n",
    "\n",
    "Now we have generated our dataset, which we can use in the Number of Speaker Estimation task. \n",
    "In summary, we did the following:\n",
    " - Rewrite `.flac` to `.wav`\n",
    " - Split `.wav` files in fragments of 5 seconds\n",
    " - Normalized loudness in every file to be 70 dB\n",
    " - Merged files together, taking into account that:\n",
    "     - Two speakers can never be in the same merged file;\n",
    "     - Classes are as balanced as possible, although perfect class balance is difficult to achieve;\n",
    "     - Normalized merged data arrays to be in the range of [-1, 1] to prevent clipping.\n",
    " - Created STFTs from the audio files, such that we can directly pass the data to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
